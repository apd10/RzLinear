{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from RzLinear import RzLinear \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rz_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ddf3ada976e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrz_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTILED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'random_numbers' is not defined"
     ]
    }
   ],
   "source": [
    "idx =rz_linear.get_idx(random_numbers, input_dim, output_dim, chunk_size, weight_size, TILED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkout the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 128x128 chunk_size: 4 weight_size: 1000  tiled: True\n",
      "output tensor(8321536., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288,\n",
       "        289, 290, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
       "        445, 446, 447, 448, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600,\n",
       "        601, 602, 603, 604, 605, 606, 597, 598, 599, 600, 601, 602, 603, 604,\n",
       "        605, 606, 607, 608, 609, 610, 611, 612, 755, 756, 757, 758, 759, 760,\n",
       "        761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 168, 169, 170, 171,\n",
       "        172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 326, 327,\n",
       "        328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341,\n",
       "        484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
       "        498, 499], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_size = 1000\n",
    "input_dim = 128\n",
    "output_dim = 128\n",
    "chunk_size = 4\n",
    "\n",
    "#hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1/np.sqrt(input_dim), 1/np.sqrt(input_dim), size=((weight_size,))).astype(np.float32)))\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "output_v = rzlinear(input_v)\n",
    "#print(output_v[:16,:16])\n",
    "#print(output_v[16:32,16:32])\n",
    "#print(output_v[16:96,:16])\n",
    "#plt.hist(np.array(output_v.detach().cpu().view(-1)))\n",
    "#np.max(np.array(output_v.detach().cpu().view(-1)))\n",
    "output_v[1,:].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(input_dim/16)):\n",
    "    for j in range(int(input_dim/16)):\n",
    "        x = output_v[i*16:(i+1)*16, j*16:(j+1)*16].reshape(-1)\n",
    "        print(x[-1] - x[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 2 weight_size: 100000  tiled: True\n",
      "output tensor(5.0167e+10, device='cuda:0')\n",
      "torch.Size([1000, 1000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "weight_size = 100000\n",
    "input_dim = 1000\n",
    "output_dim = 1000\n",
    "chunk_size = 2\n",
    "\n",
    "#hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1/np.sqrt(input_dim), 1/np.sqrt(input_dim), size=((weight_size,))).astype(np.float32)))\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "output_v = rzlinear(input_v)\n",
    "print(output_v.shape)\n",
    "\n",
    "\n",
    "plt.hist(np.array(output_v.detach().cpu()).reshape(-1), bins = int(input_dim/10))\n",
    "plt.show()\n",
    "#output_v[1,:].long()\n",
    "\n",
    "#idx = rz_linear.get_idx(rzlinear.random_numbers, input_dim, output_dim, chunk_size, weight_size, True)\n",
    "#plt.hist(np.array(idx.detach().cpu()).reshape(-1), bins = int(input_dim/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50564., 50565., 50566.,  ..., 18111., 18112., 18113.],\n",
       "        [50580., 50581., 50582.,  ..., 18127., 18128., 18129.],\n",
       "        [50596., 50597., 50598.,  ..., 18143., 18144., 18145.],\n",
       "        ...,\n",
       "        [59330., 59331., 59332.,  ..., 41719., 41720., 41721.],\n",
       "        [59346., 59347., 59348.,  ..., 41735., 41736., 41737.],\n",
       "        [59362., 59363., 59364.,  ..., 41751., 41752., 41753.]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50564, 50565, 50566,  ...,     0,     0,     0],\n",
       "        [50580, 50581, 50582,  ...,     0,     0,     0],\n",
       "        [50596, 50597, 50598,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [59330, 59331, 59332,  ...,     0,     0,     0],\n",
       "        [59346, 59347, 59348,  ...,     0,     0,     0],\n",
       "        [59362, 59363, 59364,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the correctness of forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 5 weight_size: 1000000  tiled: True\n"
     ]
    }
   ],
   "source": [
    "weight_size = 1000000\n",
    "input_dim = 1000\n",
    "output_dim = 1000\n",
    "chunk_size = 5\n",
    "\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "idx_matrix = rzlinear(input_v).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      1,      2,  ...,    997,    998,    999],\n",
       "        [   100,    101,    102,  ...,   1097,   1098,   1099],\n",
       "        [   200,    201,    202,  ...,   1197,   1198,   1199],\n",
       "        ...,\n",
       "        [ 99700,  99701,  99702,  ..., 100697, 100698, 100699],\n",
       "        [ 99800,  99801,  99802,  ..., 100797, 100798, 100799],\n",
       "        [ 99900,  99901,  99902,  ..., 100897, 100898, 100899]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_matrix.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 5 weight_size: 1000000  tiled: True\n"
     ]
    }
   ],
   "source": [
    "hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1,1, size=(weight_size,)).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_v = torch.rand((5,input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rzlinear(input_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = hashed_weight[idx_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = torch.matmul(input_v, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK\n"
     ]
    }
   ],
   "source": [
    "if torch.norm(out - ground_truth) == 0:\n",
    "    print(\"All OK\")\n",
    "else:\n",
    "    print(\"Issue in forward pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RzLinear import RzLinearFunction\n",
    "import torch\n",
    "from RzLinear import RzLinear \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2038074743,  634329019, 1825252241,  871205357,   80759397],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "TILED = True\n",
    "weight_size = 1000\n",
    "input_dim = 64\n",
    "output_dim = 64\n",
    "chunk_size = 1\n",
    "seed = 1024\n",
    "r = np.random.RandomState(seed)\n",
    "x = r.randint(0, 2038074743, (50,))\n",
    "x = x + 1*(x%2==0);\n",
    "random_numbers = torch.from_numpy(np.concatenate([np.array([2038074743]), x])).long().cuda(0) # set of 50 random numbers to use\n",
    "print(random_numbers[:5])\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32))).to(\"cuda:0\")\n",
    "input_v = torch.eye(input_dim).cuda(0)\n",
    "#hashed_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[259., 260., 261.,  ..., 594., 595., 596.],\n",
       "        [275., 276., 277.,  ..., 610., 611., 612.],\n",
       "        [291., 292., 293.,  ..., 626., 627., 628.],\n",
       "        ...,\n",
       "        [240., 241., 242.,  ..., 727., 728., 729.],\n",
       "        [256., 257., 258.,  ..., 743., 744., 745.],\n",
       "        [272., 273., 274.,  ..., 759., 760., 761.]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RzLinearFunction.forwardproxy(hashed_weight, input_v ,random_numbers, input_dim, output_dim, chunk_size, TILED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, tiled):\n",
    "    out = RzLinearFunction.forwardproxy(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size , tiled)\n",
    "    return out, torch.sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(1972224., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out, val = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED )\n",
    "torch.cuda.synchronize()\n",
    "print(\"loss\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = out * 0 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096., device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_grad, in_grad = RzLinearFunction.backwardproxy(grad, hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096., device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(wt_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.001\n",
    "_, f0 = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "hwt_grad = torch.empty_like(hashed_weight, dtype=torch.float32)\n",
    "for i in range(len(hashed_weight)):\n",
    "#for i in [10]:\n",
    "    hwt = hashed_weight.clone()\n",
    "    hwt[i] += epsilon\n",
    "    _, fi = myFunc(hwt, input_v, random_numbers, input_dim, output_dim, chunk_size,TILED)\n",
    "    hwt_grad[i] = (fi - f0) / epsilon\n",
    "    #print(i,np.float(fi.cpu()),np.float(f0.cpu()), hwt_grad[i],  wt_grad[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error norm tensor(16079.9502, device='cuda:0')\n",
      "tensor([15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 16749.9980, 15999.9990, 16249.9990, 15999.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 14999.9990, 14999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15499.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 16249.9990, 16749.9980, 16249.9990, 16249.9990, 16749.9980,\n",
      "        16249.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 16749.9980,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16749.9980, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16749.9980, 16749.9980, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 16249.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        16249.9990, 16749.9980, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 16749.9980, 16749.9980,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15499.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 14999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 14999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16749.9980,\n",
      "        15999.9990, 16749.9980, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15499.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 16749.9980, 16249.9990, 16249.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15499.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15499.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 16749.9980, 15999.9990, 16749.9980, 16749.9980,\n",
      "        15999.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 14999.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15499.9990, 15999.9990, 16249.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 14999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 16249.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 16249.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15499.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        16749.9980, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16749.9980, 16749.9980, 16749.9980,\n",
      "        16749.9980, 15999.9990, 16749.9980, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16749.9980, 15999.9990, 16749.9980, 16749.9980,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15499.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990], device='cuda:0')\n",
      "tensor([15351.2461, 15492.1709, 15581.8164, 15818.1152, 15156.3457, 15512.9531,\n",
      "        15365.6016, 15757.6953, 15227.3740, 15333.2002, 15288.6318, 15311.3955,\n",
      "        14924.3701, 15416.9648, 16067.4961, 15692.7871, 15869.9805, 15797.9131,\n",
      "        16018.2539, 15993.6416, 15867.8486, 15529.4023, 16096.5576, 15732.9199,\n",
      "        15420.8242, 15428.0596, 15745.6846, 15278.4404, 15475.2842, 15420.6797,\n",
      "        15623.2158, 15735.4502, 15432.0273, 15634.4844, 15753.6758, 15747.2402,\n",
      "        15493.9004, 15703.8330, 15512.0654, 15337.1309, 15017.2324, 14980.8574,\n",
      "        15382.6143, 15314.1641, 15212.1406, 15306.2676, 15706.4092, 15483.2568,\n",
      "        15214.8838, 15448.0635, 15807.4756, 15510.9961, 15471.8838, 15383.6504,\n",
      "        15743.9043, 15501.3750, 15057.2656, 15392.0439, 15585.1670, 15563.0049,\n",
      "        15138.5420, 15313.0176, 15317.5469, 15528.2686, 15119.4268, 15526.7881,\n",
      "        16130.8721, 15590.5713, 15434.3906, 15780.1914, 15856.9883, 15829.7744,\n",
      "        15700.1816, 15554.6201, 15980.2959, 15659.7803, 15436.7451, 15563.8535,\n",
      "        15710.5176, 15061.3584, 15441.8486, 15748.7705, 15821.6113, 15626.5811,\n",
      "        15453.5371, 15898.8027, 15830.1963, 15348.5889, 15396.1240, 15829.4678,\n",
      "        15547.4922, 14855.8135, 15145.0693, 15218.5723, 15755.3662, 15206.3037,\n",
      "        15465.9844, 15534.8018, 16110.7168, 15226.0088, 15465.8389, 15725.0439,\n",
      "        16037.6084, 15467.7549, 15794.4229, 15471.2793, 15973.8965, 15717.0020,\n",
      "        15201.9316, 15322.2520, 15736.0635, 15792.7061, 15202.1875, 15620.7744,\n",
      "        15521.8613, 15518.6279, 15011.5078, 15645.0957, 15793.2676, 15397.8555,\n",
      "        15194.5820, 15609.6982, 15487.2490, 15224.1094, 15580.1777, 15616.5186,\n",
      "        15846.9531, 15433.2539, 15404.7178, 15577.1650, 15656.8350, 15221.4316,\n",
      "        15287.5605, 15886.9775, 15852.1738, 15872.7002, 15709.7852, 16310.4453,\n",
      "        15925.7617, 15309.1250, 15224.5928, 15823.8154, 15414.0020, 14943.8750,\n",
      "        14963.2842, 15563.9434, 15690.6689, 15239.6650, 15479.2744, 15831.9854,\n",
      "        16221.0596, 15470.4668, 15573.0996, 16176.4053, 15909.6309, 15463.5244,\n",
      "        15768.9580, 15704.6113, 15811.6152, 15682.0596, 15374.0127, 15480.0596,\n",
      "        15714.5732, 15675.0586, 15433.8281, 15960.2100, 15443.0918, 15731.7227,\n",
      "        15484.5664, 15793.7451, 15505.5947, 15389.4482, 15267.4141, 15614.3359,\n",
      "        15316.5205, 15163.7207, 15615.5146, 15327.9385, 15530.8252, 15617.9414,\n",
      "        15884.9268, 15743.9463, 15673.7090, 15696.6406, 15888.1094, 16124.3359,\n",
      "        15890.8115, 16138.1543, 16098.4414, 16131.5977, 15540.4785, 15461.6982,\n",
      "        15465.9902, 16049.6553, 15204.8838, 15297.3311, 15366.5742, 15821.2920,\n",
      "        15478.3223, 15375.0400, 15770.6162, 16068.3906, 15993.2158, 15592.0361,\n",
      "        15879.5684, 16253.1172, 15551.4277, 15164.3242, 15733.9609, 15902.4727,\n",
      "        15693.7061, 15708.1729, 15713.2451, 15484.6309, 15926.5049, 15696.5547,\n",
      "        15870.1553, 16217.7764, 15743.8193, 15571.9961, 15573.9209, 15589.5361,\n",
      "        15458.6973, 15357.4209, 15468.8906, 15534.7842, 15479.4639, 14971.7500,\n",
      "        15591.9336, 15285.4082, 15581.4189, 15260.9697, 16099.5322, 15998.8203,\n",
      "        15930.6104, 15791.7354, 15885.7090, 15747.5762, 15655.2812, 15591.6143,\n",
      "        15753.6104, 15825.7734, 15317.5010, 15472.8154, 15600.4883, 15907.9893,\n",
      "        15318.6455, 15541.6758, 15633.6025, 15847.7568, 15343.3467, 15345.3691,\n",
      "        15668.4648, 15848.9590, 15652.3584, 15867.8203, 16163.6250, 15949.3789,\n",
      "        15294.6992, 15006.9238, 15784.3584, 15588.6006, 15557.1484, 15696.6523,\n",
      "        15827.2188, 15301.0283, 15864.3154, 15763.5547, 15818.1582, 15963.4307,\n",
      "        15744.2734, 15307.3984, 15514.2314, 15246.4570, 14931.5869, 15025.6162,\n",
      "        15160.1230, 15170.4697, 15182.5703, 14910.3594, 15504.2676, 15215.8145,\n",
      "        15472.7295, 15297.4717, 16240.2783, 15614.6758, 15770.0352, 15727.3184,\n",
      "        15716.2490, 15396.5537, 15535.5967, 15670.4531, 15677.7842, 15369.3418,\n",
      "        15265.5078, 15431.2832, 15775.5908, 15579.6260, 15463.1934, 15759.7275,\n",
      "        15828.1201, 15733.5908, 15575.3359, 15447.4463, 15650.4316, 15574.7295,\n",
      "        15241.1523, 15669.5693, 15809.5439, 15533.8350, 15464.7021, 15350.2324,\n",
      "        15682.7783, 15719.1914, 15740.8193, 15854.7461, 15823.9814, 15524.5195,\n",
      "        15599.3633, 15902.2881, 15849.2949, 15895.7637, 15621.9932, 15536.4287,\n",
      "        15390.4131, 15350.0830, 14715.1416, 15304.5684, 15231.1475, 15029.0459,\n",
      "        15005.2549, 15354.7471, 15677.1973, 15363.0811, 15577.0107, 15589.8867,\n",
      "        16398.6777, 15515.7666, 15448.1279, 15664.1006, 15708.1836, 15197.0781,\n",
      "        15679.9102, 15841.0029, 15816.9492, 15670.9150, 15649.6924, 15746.6865,\n",
      "        15994.2773, 15672.0479, 15662.7881, 15877.5391, 15737.9727, 15445.5078,\n",
      "        15787.6758, 15569.2275, 15635.7637, 15296.0195, 15435.3291, 15566.2646,\n",
      "        15289.8809, 14964.1719, 15427.1836, 15584.1680, 15462.5615, 15662.0068,\n",
      "        15743.5166, 16034.5234, 15562.9746, 15512.8916, 15539.8574, 16287.8066,\n",
      "        15802.7412, 15636.7432, 15734.1729, 16191.8965, 15574.2197, 15556.7725,\n",
      "        15535.1758, 16088.6504, 15515.1719, 15217.5498, 15367.9229, 15883.7793,\n",
      "        15830.6445, 15329.2627, 15502.2627, 15792.5332, 16008.9395, 15230.8203,\n",
      "        15074.3047, 15306.4717, 15284.2939, 14666.8193, 15249.5762, 15561.0166,\n",
      "        15523.9805, 15585.8564, 15565.1211, 15645.8301, 16195.6553, 15854.2324,\n",
      "        15536.4062, 16104.2021, 15987.4922, 15487.6543, 15605.2930, 15593.1973,\n",
      "        15555.3145, 15184.6113, 15325.6924, 15365.2236, 15621.6387, 15168.2256,\n",
      "        15707.4209, 15835.0645, 15852.4043, 15598.6016, 15870.6719, 15964.8125,\n",
      "        15471.6641, 15320.3789, 15554.6670, 16166.2188, 15476.1064, 15456.5039,\n",
      "        15653.1064, 16009.1982, 15181.4082, 15708.8359, 15764.2637, 16082.1729,\n",
      "        15532.0195, 15756.4395, 15591.6104, 15986.3848, 15648.2959, 15410.8945,\n",
      "        15460.1943, 15824.3682, 15573.9160, 15277.1611, 15340.2383, 15451.8877,\n",
      "        15256.6260, 14984.3066, 15579.9473, 15731.2871, 15527.9395, 15774.5977,\n",
      "        15742.3301, 15912.0977, 15958.6367, 15986.8271, 15849.0088, 16246.3350,\n",
      "        15800.9453, 15884.5449, 15761.1523, 15553.9717, 15363.3486, 15457.3418,\n",
      "        15534.3535, 15447.1260, 15638.3633, 15242.0566, 15777.7256, 15523.0146,\n",
      "        15575.7500, 15364.2012, 15985.2432, 15575.4229, 15378.9590, 15267.2998,\n",
      "        15517.6836, 15758.7852, 15268.4629, 15430.5811, 15686.8252, 15896.5020,\n",
      "        15155.0039, 15781.1094, 15915.8867, 16013.1943, 15516.0947, 15840.6660,\n",
      "        15755.4004, 16116.9268, 15530.0273, 15265.6025, 15494.2910, 15821.8896,\n",
      "        15319.6514, 15382.1133, 15555.8809, 15358.0254, 15055.5928, 15179.4385,\n",
      "        15354.8750, 15736.3848, 15354.9141, 15653.5273, 15824.2061, 15951.1924,\n",
      "        15586.3535, 15783.1270, 15559.6182, 16037.7734, 15409.5820, 15596.5586,\n",
      "        15595.6885, 15694.1807, 15210.7725, 15300.8379, 15695.8945, 15696.4219,\n",
      "        16037.6553, 15467.3281, 16040.2188, 15569.0918, 15555.1904, 15124.7812,\n",
      "        15910.2549, 15336.6455, 15289.8271, 15217.8965, 15498.0439, 15278.2100,\n",
      "        15410.8369, 15513.7266, 15712.5684, 15947.7393, 15739.2383, 15894.9922,\n",
      "        16234.7666, 16005.3252, 15826.0879, 15954.6982, 15828.2109, 15909.6367,\n",
      "        15389.9795, 15143.1396, 15305.7275, 15133.7119, 15120.9365, 15321.4492,\n",
      "        15521.3711, 15198.9619, 15391.4863, 15191.0967, 15536.3047, 15439.1465,\n",
      "        15452.8105, 15351.9473, 15777.3398, 15450.5830, 15324.8838, 15623.3867,\n",
      "        15562.5576, 15667.9521, 15610.6582, 16059.0127, 15763.3770, 15620.8799,\n",
      "        15278.1689, 15665.4238, 15571.1592, 15460.5586, 15887.3711, 15872.6387,\n",
      "        15979.7080, 15478.0186, 15503.5586, 15357.7080, 15800.1836, 15274.6377,\n",
      "        15068.6113, 15173.4756, 15336.5850, 15095.3682, 15307.2676, 15658.3828,\n",
      "        15737.2754, 16208.9863, 16091.2832, 15718.9658, 16256.3779, 16037.3965,\n",
      "        15607.6064, 15817.1963, 16024.1797, 15768.7109, 15716.8643, 15251.1055,\n",
      "        15528.3496, 15393.9092, 15701.6084, 15459.7578, 15874.9131, 15304.0439,\n",
      "        15820.8955, 15326.6367, 15766.7100, 15461.8809, 15676.0186, 15536.0293,\n",
      "        15773.5879, 15013.5674, 15162.6221, 15607.6299, 15340.5410, 15027.9219,\n",
      "        15534.0039, 16184.4355, 15547.3057, 15553.4512, 15627.2549, 15880.9639,\n",
      "        15508.5820, 15594.9287, 15671.9268, 15814.2773, 15763.9102, 15260.1318,\n",
      "        15264.4131, 15474.0459, 15577.8926, 15232.6074, 15348.0674, 15395.2217,\n",
      "        15241.8242, 15125.3936, 15246.9004, 15862.7607, 15711.9297, 15919.8252,\n",
      "        16045.6260, 15810.9746, 15896.2148, 15591.6670, 15407.9424, 15709.7295,\n",
      "        15642.7207, 15582.1592, 15684.2178, 15658.5781, 15417.4297, 15824.0684,\n",
      "        15924.0576, 15723.4473, 15951.9688, 15657.4023, 15614.0645, 15493.0430,\n",
      "        15477.0996, 15237.5098, 15570.7588, 15488.8867, 15518.6074, 15223.3262,\n",
      "        15290.3301, 15720.5371, 15537.8047, 15354.2148, 15639.1943, 16263.1982,\n",
      "        15622.0605, 15648.7285, 15727.2168, 15969.5059, 15615.6250, 15755.3018,\n",
      "        15536.9678, 15786.3691, 15787.8232, 15142.1133, 15374.9834, 15770.5557,\n",
      "        15777.0312, 15294.7275, 15605.9219, 15700.2158, 15771.7656, 15635.4580,\n",
      "        15541.6533, 16234.4473, 15849.8096, 15734.5957, 15558.3730, 15809.9814,\n",
      "        15555.0859, 15801.1514, 15481.6289, 16026.7373, 15855.5215, 15977.6670,\n",
      "        15852.2188, 15890.4043, 15372.6152, 16062.9697, 16008.0293, 15945.5977,\n",
      "        16074.8691, 16109.8838, 15903.5576, 15743.4775, 15560.7324, 15341.3535,\n",
      "        15574.2588, 15265.2920, 14976.3379, 14858.1494, 15487.1807, 15520.5234,\n",
      "        15332.3027, 15432.8496, 15925.4004, 16041.3535, 15606.3164, 15520.1982,\n",
      "        16027.4043, 15821.2168, 15702.5146, 15727.0117, 15921.9795, 15889.2578,\n",
      "        15823.6777, 15297.5322, 15800.2949, 15710.4795, 15826.0781, 15513.7617,\n",
      "        16107.9443, 15789.8926, 15932.5928, 15672.6943, 15766.3428, 16027.9014,\n",
      "        15455.7275, 15240.4082, 15450.0938, 15637.2275, 15156.9043, 15563.6201,\n",
      "        15671.8682, 16049.0820, 15558.6807, 15769.9697, 15730.9180, 15653.9150,\n",
      "        15357.8428, 15862.3936, 15960.4004, 15814.8721, 15757.3623, 15847.5400,\n",
      "        15655.5684, 15537.4492, 15556.8135, 15239.6533, 15617.4336, 15432.2383,\n",
      "        15121.8359, 15112.3311, 15570.5781, 15490.8408, 15223.7461, 15494.1318,\n",
      "        15716.1699, 15764.0469, 15481.5430, 15283.3477, 15749.3652, 15367.7422,\n",
      "        15265.2393, 15387.5176, 15729.3525, 15415.9668, 15542.6396, 15328.3896,\n",
      "        15701.1484, 15647.3799, 15773.8184, 15552.5820, 16399.8145, 15654.0195,\n",
      "        15828.4805, 15512.7666, 15665.0664, 15568.8594, 15209.0879, 14958.3223,\n",
      "        15516.7441, 15373.3818, 14920.1963, 15381.9375, 15539.5596, 15615.6855,\n",
      "        15568.9336, 15898.2354, 15770.2949, 15571.6387, 15623.1299, 15781.5947,\n",
      "        15959.0586, 15881.6055, 15787.3428, 15840.2363, 15770.2393, 15518.2051,\n",
      "        15499.7480, 15594.3594, 15839.1387, 15397.3799, 15160.9854, 15603.9980,\n",
      "        15660.2441, 15527.7363, 15208.4248, 15800.3633, 15597.3213, 15700.5186,\n",
      "        15360.9160, 15398.4258, 15460.3037, 15195.1006, 14779.6699, 15429.4072,\n",
      "        15474.1094, 15503.7354, 15715.7959, 15725.7520, 15622.4619, 15796.2627,\n",
      "        16070.2842, 15817.8877, 16329.7686, 15665.4512, 15871.4014, 15499.7217,\n",
      "        15561.0869, 15262.9307, 15479.0322, 15345.9922, 15754.8760, 15458.4453,\n",
      "        15042.6484, 15616.4775, 15652.0439, 15297.4268, 15295.4521, 15735.0430,\n",
      "        15752.2402, 15334.8105, 15397.4141, 15510.5752, 15873.0635, 15446.0010,\n",
      "        15526.6230, 15670.8311, 16004.4297, 15575.5586, 15398.3467, 15752.6289,\n",
      "        16007.2129, 15747.8535, 15562.2314, 15908.8516, 15960.9424, 15840.1709,\n",
      "        15275.4053, 15703.5791, 15856.1045, 16014.2471, 15553.4688, 15891.0518,\n",
      "        15861.7100, 15722.3643, 15039.7832, 15665.1006, 15505.8984, 15690.9492,\n",
      "        15886.3994, 15930.2754, 15572.7598, 16004.9854, 16128.1816, 15956.9912,\n",
      "        16019.1602, 15800.7783, 15696.1826, 15374.7100, 15460.6172, 14635.6094,\n",
      "        15279.0977, 15276.8467, 15360.0088, 15283.8887, 15439.7021, 15786.7188,\n",
      "        15440.4990, 15467.2734, 15588.0166, 16046.6836, 15661.4121, 15519.3779,\n",
      "        15661.2666, 15513.6084, 15546.3037, 15545.4502, 15754.5098, 15763.7314,\n",
      "        16020.9609, 15554.7207, 15775.8857, 16031.1357, 16125.0293, 15949.0801,\n",
      "        16094.2637, 15886.3184, 15983.4482, 15848.4316, 15216.1475, 15529.6914,\n",
      "        15503.2803, 15545.5049, 15213.9053, 15848.6904, 15578.8223, 15533.1084,\n",
      "        15153.6748, 15815.1982, 15773.0908, 15819.8887, 15902.0684, 16132.0264,\n",
      "        15488.7090, 15858.1924, 15865.8643, 15917.3867, 15671.8799, 15853.6406,\n",
      "        15805.5176, 15606.7715, 15630.3770, 15195.9375, 15630.0908, 15611.3350,\n",
      "        15436.0186, 15692.0010, 15768.8242, 16058.4229, 15507.3828, 15843.7080,\n",
      "        15704.6270, 16308.5713, 15523.7520, 15399.0898, 15461.5264, 15529.2881,\n",
      "        15249.3750, 15417.7793, 15964.7100, 16022.9365, 16179.8252, 15785.7148,\n",
      "        16097.7822, 16371.5088, 16060.6807, 15565.0967, 16213.9736, 15974.9707,\n",
      "        15895.4863, 15635.2588, 15689.6182, 15727.2402, 15491.9756, 15281.8408,\n",
      "        15355.0352, 15759.6758, 15273.2012, 15476.1182, 15526.8135, 16032.5225,\n",
      "        15797.0273, 15971.4600, 15911.3086, 15817.7324, 15251.4404, 15603.3232,\n",
      "        15664.0088, 15607.1123, 15527.6025, 15714.4639, 15661.4434, 15721.4102,\n",
      "        15507.7402, 15494.7793, 15546.4121, 15707.9639], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (\"error norm\", torch.norm(hwt_grad - wt_grad))\n",
    "#print(torch.max(torch.abs(hwt_grad - wt_grad)))\n",
    "#print(hwt_grad[hwt_grad != 0][:10])\n",
    "#print(wt_grad[hwt_grad != 0][:10])\n",
    "print(hwt_grad[wt_grad != 0])\n",
    "print(wt_grad[wt_grad != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-4\n",
    "_, f0 = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED )\n",
    "int_grad = torch.empty_like(input_v)\n",
    "for i in range(int_grad.shape[0]):\n",
    "    for j in range(int_grad.shape[1]):\n",
    "        inputt = input_v.clone()\n",
    "        inputt[i][j] += epsilon\n",
    "        _, fi = myFunc(hashed_weight, inputt, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "        int_grad[i][j] = (fi - f0) / epsilon\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error norm tensor(213606.0938, device='cuda:0')\n",
      "tensor([[10000., 10000., 10000., 10000., 10000.],\n",
      "        [15000., 15000., 15000., 15000., 15000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.]], device='cuda:0')\n",
      "tensor([[7991.0576, 8006.8306, 8022.6045, 7998.4473, 8014.2192],\n",
      "        [8395.5645, 8412.1270, 8428.6875, 8403.6836, 8420.2451],\n",
      "        [7688.5938, 7703.7388, 7718.8853, 7694.8872, 7710.0332],\n",
      "        [8171.5391, 8187.7100, 8203.8711, 8179.2729, 8195.4365],\n",
      "        [7735.8730, 7751.1753, 7766.4717, 7742.8701, 7758.1646]],\n",
      "       device='cuda:0')\n",
      "tensor(7592.6670, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (\"error norm\", torch.norm(int_grad - in_grad))\n",
    "print(int_grad[:5,:5])\n",
    "print(in_grad[:5,:5])\n",
    "print(torch.max(torch.abs(int_grad - in_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
